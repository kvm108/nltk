{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 3\n",
    "# http://www.nltk.org/book/ch03.html\n",
    "\n",
    "\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 \n",
    "#Step 0 : Get data \n",
    "\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type <class 'str'>\n",
      "Len 1176965\n",
      "Some text ﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "data_type = type(raw) #string\n",
    "length = len(raw)\n",
    "tiny_dump = raw[:75]\n",
    "\n",
    "print('\\nType {}\\nLen {}\\nSome text {} \\n'.format(data_type, length, tiny_dump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize\n",
    "\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens \n",
      " Type <class 'list'> \n",
      " Length 257726 \n",
      " Dump ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "type_tokens = type(tokens)\n",
    "tok_len = len(tokens)\n",
    "tok_dump = tokens[:10]\n",
    "\n",
    "print('Tokens \\n Type {} \\n Length {} \\n Dump {} \\n'.format(type_tokens, tok_len, tok_dump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1.a : Convert to Text\n",
    "\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "type(text)\n",
    "text[1024:1062]\n",
    "collocations = text.collocations()\n",
    "print(collocations)\n",
    "\n",
    "#Collocations are expressions of multiple words which commonly co-occur.\n",
    "\n",
    "#->the habitual juxtaposition of a particular word with another word or words with a frequency greater than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops: Project Glutenberg ocours in collocations. Why: Header, Footer stuff in PG. remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5336, 1157810)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = raw.find(\"PART I\")\n",
    "end = raw.rfind(\"End of Project Gutenberg’s Crime and Punishment\")\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[start: end]\n",
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<head>\\r\\n<title>BBC NEWS | Health | Blondes \\''"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://news.bbc.co.uk/2/hi/health/2284783.stm\n",
    "\n",
    "url = 'http://news.bbc.co.uk/2/hi/health/2284783.stm'\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html, \"html5lib\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens\n",
    "\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', 409), ('t', 276), ('a', 246), ('i', 236), ('s', 234)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "rawt = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "print(fdist.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
